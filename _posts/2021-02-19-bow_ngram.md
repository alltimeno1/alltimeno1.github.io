---
title: Bag of Words와 N-Gram
tags: 자연어

---

**Bag of Words**

**자연어를 단어의 빈도수로 벡터화하여 분석하는 기법**

<br>

<img src="https://user-images.githubusercontent.com/71831714/108496157-0ca6b680-72ed-11eb-9b1d-a59215c85406.png" width="40%" height="40%">

<br>

**The cat is on the table** : [0, 1, 0, 1, 0, 1, 1, 1]

```
0x0 + 0x1 + 1x0 + 1x1 + 0x0 + 1x1 + 1x1 + 1x1 = 4
```

유사한 2개의 문장들을 BoW로 표현하고 유사도를 간단하게 표현할 수 있다.

<br>

BoW의 단점

1. Sparsity

   단어의 수가 많아지면 벡터의 차원도 그에 정비례해서 커지기 때문에 데이터가 커질수록 많은 연산량을 필요로 한다.

   

2. Frequent words has more power

   많이 출현한 단어가 더 큰 힘을 가지게 된다. 만약 의미없는 단어가 많이 사용됐다면 우리가 원하는 결과를 얻기 힘들 수 있다.
   
   **The The The The The The** : [0, 0, 0, 0, 0, 0, 0, 6]

   이 문장과 위의 문장간의 유사도를 구하면 6이라는 다소 부정확한 결과가 나온다.

   

3. Ignoring word orders

   문장에서 단어의 순서를 고려하지 않는다. 예를 들어, run home과 home run은 전혀 다른 의미이지만 BoW를 사용하면 같은 벡터로 표현된다.

   

4. Out of Vocabulary

   오타나 줄임말 등 처음 보는 단어들에 대한 처리를 할 수 없다.

<br>

**N-Gram**

**단어 혹은 문자(음절)을 n개 단위로 묶어 토큰화하는 기법**

<br>

<img src="https://user-images.githubusercontent.com/71831714/108502953-f7368a00-72f6-11eb-8bbf-906ed93b688b.png" width="40%" height="40%">

<br>

"These are dogs and not cats"이라는 문장을 BoW로 표현하면 not이 문맥적으로 cats을 가르키는지 dogs를 가르키는지 알 수 없다. 

같은 문장을 2-Gram으로 표현하면 These are, are dogs, dogs and, and not, not cats로 분리되기 때문에 단어의 순서가 부분적으로 반영된다.

n이 너무 크면 훈련 Corpus에서 해당 n-gram을 카운트하지 못 하는 OoV 문제가 발생하고 너무 작으면 예측의 정확도가 떨어지게 된다. 

다음에 등장할 단어나 오타의 예측에 N-gram을 활용할 수 있다.
